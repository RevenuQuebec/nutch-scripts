#!/bin/sh

SOURCE_DIR=$( dirname "$0" )
DIR=$( cd "$SOURCE_DIR" && pwd )

. $DIR/common.sh

MUST_REMAIN_SPACE=500000 # 500 Mb
SCRIPTS_DIR=`dirname "${0}"`
CRAWL_SCRIPT=$SCRIPTS_DIR/crawl
REINDEX_SCRIPT=$SCRIPTS_DIR/reindex
FIXSEGMENTS_SCRIPT=$SCRIPTS_DIR/fixSegments
CURWD=`pwd`

MAX_FETCH_TIME=2
MAX_BACKUP_TIME=1

LOGS_DIR=$1
BACKUPS_DIR=$2

do_core() {
    CORE_NAME=$1

    CRAWLDB_DIR=${CORES_HOME}/${CORE_NAME}/crawldb
    if [ ! -e "${CRAWLDB_DIR}" ]; then
        mkdir ${CRAWLDB_DIR}
    else
        $FIXSEGMENTS_SCRIPT ${CORE_NAME}
    fi

    CRAWLDB_SIZE=`du --max-depth=0 $CRAWLDB_DIR | awk '{print $1}'`
    SPACE_LEFT=`df -k $BACKUPS_DIR | tail -1 | awk '{print $4}'`
    REMAINING_SIZE=$(($SPACE_LEFT - $CRAWLDB_SIZE))

    BACKUP_OUT_LOG=$LOGS_DIR/${DATE}-OUT-backup-${1}.log
    BACKUP_ERR_LOG=$LOGS_DIR/${DATE}-ERROR-backup-${1}.log
    CRAWL_OUT_LOG=$LOGS_DIR/${DATE}-OUT-crawl-${1}.log
    CRAWL_ERR_LOG=$LOGS_DIR/${DATE}-ERROR-crawl-${1}.log
    REINDEX_OUT_LOG=$LOGS_DIR/${DATE}-OUT-reindex-${1}.log
    REINDEX_ERR_LOG=$LOGS_DIR/${DATE}-ERROR-reindex-${1}.log

    # Create a backup of the crawl DB, just in case
    if [ -e "${CORES_HOME}/${CORE_NAME}/crawldb" ]; then
        if [ $REMAINING_SIZE -le $MUST_REMAIN_SPACE ]; then
            echo "${1}: Not enough space to create the backup";
            return 1;
        fi
        tar -jcf $BACKUPS_DIR/crawl-${1}-$DATE.tar.bz2 -C ${CORES_HOME}/${CORE_NAME} crawldb > $BACKUP_OUT_LOG 2> $BACKUP_ERR_LOG;
        RETCODE=$?
        # Check if the backup failed
        if [ $RETCODE -ne 0 ]; then
            echo "${1}: Backup failed (${RETCODE})";
            rm -rf $BACKUPS_DIR/crawl-${1}-$DATE;
            return 1;
        fi
    fi

    # Remove old segments
    if [ -e $CRAWLDB_DIR/segments ]; then
        cd $CRAWLDB_DIR/segments && find . -maxdepth 1 -mtime +${MAX_FETCH_TIME} -exec rm -rf '{}' \;
        cd $CURWD
    fi

    # Crawl the website
    $CRAWL_SCRIPT $1 > $CRAWL_OUT_LOG 2> $CRAWL_ERR_LOG;
    RETCODE=$?
    # If an error happened while crawling the website, we must restore the crawldb directory from a backup
    if [ $RETCODE -ne 0 ]; then
        if [ -e $BACKUPS_DIR/crawl-${1}-$DATE.tar.bz2 ]; then
            echo "${1}: Crawl failed, restoring backup (${RETCODE})";
            mv ${CORES_HOME}/${CORE_NAME}/crawldb ${CORES_HOME}/${CORE_NAME}/crawldb.old;
            tar -C ${CORES_HOME}/${CORE_NAME} -jxf $BACKUPS_DIR/crawl-${1}-$DATE.tar.bz2;
            rm -rf ${CORES_HOME}/${CORE_NAME}/crawldb.old;
            return 1;
        fi
    else
        # Remove old log files
        find $LOGS_DIR -type f -mtime +${MAX_FETCH_TIME} -exec rm -rf {} \; &> /dev/null
        # Remove old backups
        find $BACKUPS_DIR -type f -mtime +${MAX_BACKUP_TIME} -exec rm -rf {} \; &> /dev/null

        # Reindex all pages
        $REINDEX_SCRIPT $1 > $REINDEX_OUT_LOG 2> $REINDEX_ERR_LOG
        RETCODE=$?
        # Check if reindexation failed
        if [ $RETCODE -ne 0 ]; then
            echo "${1}: Reindexation failed (${RETCODE})";
            return 1;
        fi
    fi
}

check_status() {
    RETCODE=$?;
    if [ $RETCODE -ne 0 ]; then
        exit $RETCODE;
    fi
}

cleanup() {
    # The Hadoop tmp directory can take all available space
    HADOOP_TMP_DIR=/tmp/hadoop-$USER
    if [ -e "$HADOOP_TMP_DIR" ]; then
        rm -rf $HADOOP_TMP_DIR;
    fi

    NUTCH_HOME=${CORES_HOME}/${1}/apache-nutch/
    rm -f ${NUTCH_HOME}/logs/hadoop.*;
}

if [ "$LOGS_DIR" = "" ] || [ "$BACKUPS_DIR" = "" ]; then
    echo "You must provide the path to the logs and backups directories (ex: ./search-cron ./logs ./backups)"
else
    if [ -e "${CORES_HOME}/search.pid" ]; then
        SEARCH_PID=`cat ${CORES_HOME}/search.pid`
        IS_RUNNING=`kill -0 $SEARCH_PID`
        if [ "$IS_RUNNING" = "0" ]; then
            echo "Script already running, exiting";
            exit 1;
        fi
    else
        echo $$ > ${CORES_HOME}/search.pid;
    fi

    if [ ! -e $LOGS_DIR ]; then
        mkdir -p $LOGS_DIR; check_status;
    fi
    if [ ! -e $BACKUPS_DIR ]; then
        mkdir -p $BACKUPS_DIR; check_status;
    fi

    DATE=`date +\%Y-\%m-\%d-\%H\%M`

    cleanup rq-fr;
    do_core rq-fr;
    RETCODE_FR=$?
    cleanup rq-fr;

    cleanup rq-en;
    do_core rq-en;
    RETCODE_EN=$?
    cleanup rq-en;

    # Delete empty log files
    find $LOGS_DIR -type f -size 0 -exec rm -f '{}' \;

    rm -f ${CORES_HOME}/search.pid;

    if [ "$RETCODE_FR" -ne "0" ]; then
        exit 1;
    fi
    if [ "$RETCODE_EN" -ne "0" ]; then
        exit 1;
    fi
fi
